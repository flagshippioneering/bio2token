n_layers_encoder: 3
n_layers_decoder: 6
loss_type: mse
vocab_size: small
use_fsq: True
max_len: 4192 
manual_masking: True
pad_track: False
nan_track: False
freeze_encoder: False
node_hidden_dims_s: 128
compute_tm_score: True
recenter: True
mamba_encoder:
  n_layer: 2
  d_model: 128
  vocab_size: 4097
  d_intermediate: 0
  rms_norm: True
  residual_in_fp32: True
  fused_add_norm: True
  pad_vocab_size_multiple: 1
  tie_embeddings: True
  pretrained_model: None
  ssm_cfg: 
    layer: Mamba1
mamba_decoder:
  n_layer: 4
  d_model: 128
  vocab_size: 4097
  d_intermediate: 0
  rms_norm: True
  residual_in_fp32: True
  fused_add_norm: True
  pad_vocab_size_multiple: 1
  tie_embeddings: True
  pretrained_model: None
  ssm_cfg: 
    layer: Mamba1
